
\documentclass[11pt]{article}

%% WRY has commented out some unused packages %%
%% If needed, activate these by uncommenting
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
%\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
\geometry{a4paper,left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
%\geometry{landscape}                % Activate for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent

%for figures
%\usepackage{graphicx}

\usepackage{color}
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}
%% for graphics this one is also OK:
\usepackage{epsfig}

%% AMS mathsymbols are enabled with
\usepackage{amssymb,amsmath}

%% more options in enumerate
\usepackage{enumerate}
\usepackage{enumitem}

%% insert code
\usepackage{listings}

\usepackage[utf8]{inputenc}

\usepackage{hyperref}

%% colors
\usepackage{graphicx,xcolor,lipsum}


\usepackage{mathtools}

\usepackage{graphicx}
\newcommand*{\matminus}{%
  \leavevmode
  \hphantom{0}%
  \llap{%
    \settowidth{\dimen0 }{$0$}%
    \resizebox{1.1\dimen0 }{\height}{$-$}%
  }%
}

\title{Notes on computational fluid dynamics}
\author{Cesar B Rocha}
\date{\today}

\begin{document}

\include{mysymbols}

\maketitle

This are notes I've been writing for my self study as part of the class ``Computational Fluid Dynamics'' (MAE 290C), taught by Prof. Juan C. del Alamo in Winter 2015. I'm also including material beyond the class syllabus.


I claim no originality to the content of these notes.  In particular, I'm loosely following del Alamo's class notes, and the following books

\begin{itemize}

    \item \textit{Spectral methods: Fundamentals in single domains} by Canuto et al.;

    \item \textit{Numerical Renaissance} by Bewley;

    \item \textit{Numerical Methods for Fluid Dynamics: With applications 
        to Geophysics} by Durran.

    \item \textit{Numerical Methods for Conservation Laws} by LeVeque.

\end{itemize}

\section{Introduction}
We will study methods for solving  the Navier-Stokes (NS) equations

\beq
\label{eq:ns}
\p_t \vec{u} + \vec{u}\cdot \nabla \vec{u} = -\frac{\nabla p}{\rho} + \nu \lap \vec{u}\com
\eeq
where the laplacian is
\beq
\label{eq:lap_defn}
\lap \defn \nabla \cdot \nabla
\eeq
The momentum equation \eqref{eq:ns} is complemented by the conservation of mass
\beq
\label{eq:cons_mass}
\nabla\cdot\vec{u} = 0\per
\eeq
To close the system, we will also need a thermodynamic equation, which we will introduce later.

The advective term $\vec{u}\cdot\nabla\vec{u}$ gives a hyperbolic flavor to the NS equations. Its quadratic non-linearity typically prohibits analytic solutions. It gives rise to fascinating phenomena like turbulence. In contrast the linear viscous term $\nu\lap\vec{u}$ gives a parabolic flavour the NS equations.

\section{Temporal discretizations of the NS equations}
We consider a one-dimensional linear version of \eqref{eq:ns}, and drop the pressure gradient term for simplicity,
\beq
\label{eq:oned_ns}
\p_t u + c \,\p_x u = \nu \p_{xx}^2 u\com
\eeq
where $c$ is a constant speed. Fourier transforming in the $x$-direction we obtain
\beq
\label{eq:oned_ns_h}
\p_t\hat{u} =\underbrace{ -(\ii c\,k + \nu k^2)}_{\defn \lambda} \hat{u}\com
\eeq
where $k$ is the wavenumber. \eqref{eq:oned_ns} is an eigenproblem with eigenvalue $\lambda$. Decaying solutions have $\Re\left(\lambda\right) < 0$, and numerical schemes must represent this behavior. Figure \ref{fig:eigs_nu} shows the eigenvalue as a function of $\nu$. A $\nu$ increases the real part of the eigenvalue becomes more negative, whereas for very small $\nu$ the eigenvalue are almost purely imaginary. This example illustrates one of the challenges for time marching numerical schemes. These schemes should have stability regions that comprise large negative real number and the imaginary axis. 

\begin{figure}[ht]
\begin{center}
\includegraphics[width=19pc,angle=0]{figs/eigs_linear_1d_ns.eps}
\end{center}
\caption{The eigenvalue $\lambda$ as function of the parameter $\nu$ with fixed speed $c = .5$.}
\label{fig:eigs_nu}
\end{figure}

One may be tempted to use implicit methods since they are unconditionally stable. However, from an implementation point of view, the implicit methods applied to the non-linear terms in \eqref{eq:ns} would lead to non-linear algebraic equations to be solved every time step, which can be very computationally costly. 

For the advective terms we typically use two main schemes:

\begin{itemize}
\item Multi-step (e.g. Adams-Bashfort methods);
    \item Single-step, multiple sub-steps (e.g. Runge-Kutta methods).
\end{itemize}
Adams-Bashforth (AB) methods are more memory demanding, whereas Runge-Kutta (RK) schemes require more flops. The most efficient method depends on computer architecture. In recent computers memory, not CPU, is the main limitations, so we may use RK methods.


\subsection{Runge-Kutta methods}
Runge-Kutta (RK) are explicit, single-step, multi-sub-steps, schemes. The general form of an RK scheme for a non-linear equation 
\beq
\label{eq:gov_eqn}
\p_t u = F(u,t)\com
\eeq
is
\beq
\label{eq:rk_general}
u^{n+1} = u^n + \dt \left(b_1 k_1 + \ldots + b_M k_M\right)
\eeq

\begin{align}
    \label{eq:rk_ks}
    k_1 &= F(u^n,t^n)\com \nonumber \\
k_2 &= F(u^n+a_{21}k_1\dt,t^n + c_2 \dt)\com \nonumber \\
    &\vdots \nonumber \\
    k_{M} &= F(u^n+a_{21}k_1\dt + \ldots + a_{M,M-1}k_{M-1}\dt,t^n , c_2 \dt + \ldots + c_M\dt)\per
\end{align}


To systematically keep track of the coefficients, we introduce the Butcher table
\begin{table}
\label{butcher_table}
\caption{The Butcher table.}
\centering
\begin{tabular}{l| c c c c c c c }
    c$_2$ & a$_{2,1}$ & 0 & \ldots & & 0 \\
    c$_3$ & a$_{3,1}$ & a$_{3,2}$ & 0 & \ldots & $\vdots$ \\
    $\vdots$ & $\vdots$ & $\ddots$ & $\ddots$  \\
    c$_M$ & a$_{M,1}$ & a$_{M,2}$ & &\ldots &a$_{M,M-1}$\\
\hline
\end{tabular}
\end{table}

\subsubsection{RK2, the simplest example}
With M=2 we have
\beq
\label{eq:rk2}
u^{n+1} = u^n + \dt \left(b_1 k_1 + b_2k_2\right)\com
\eeq
with
\begin{align}
k_1 &= F(u^n,t^n ) \com \nonumber \\
k_2 &= F(u^n + a_{2,1} k_1 \dt, t^n +c_2 \dt )\per
\end{align}
There are four parameters $a_{2,1}$, $b_1$, $b_2$, and $c_2$. We choose these parameters to maximize the order of accuracy of the scheme. We expand $k_2$ about $k_1$
\beq
\label{eq:k2_about_k1}
k_2 = k_1 + a_{21} k_1 \dt \,\p_u F (u^n,t^n) + c_2 \dt \p_t \, F + \ord(\dt^2)\per
\eeq
Thus
\beq
\label{eq:un_rk2}
u^{n+1} = u^n + \dt\left[(b_1 + b_2) F(u^n,t^n) + b_2 a_{2,1} \dt\, \p_u F(u^n,t^n) + b_2 c_2 \dt\, \p_t F(u^n,t^n) \right]
\eeq
The Taylor expansion of $u^{n+1}$ about $u^n$ is
\begin{align}
\label{eq:un_taylor}
u^{n+1} = u^n + \dt& \p_t u |^n + \frac{\dt^2}{2} \p_{tt}^n u | ^n + \ord(\dt^3) =\nonumber\\ & u^n + \dt \p_t u |^n +\p_t F(u^n,t^n)+\p_u F(u^n,t^n)F(u^n,t^n) + \ord(\dt^3)  \com
\end{align}
where the equality follows from the governing equation \eqref{eq:gov_eqn}. Matching terms in \eqref{eq:un_taylor} and \eqref{eq:un_rk2} gives
\beq
\label{eq:sys_rk2}
\begin{cases}
b_1 + b_2 = 1; \\
b_2 c_2 = \frac{1}{2};\\
b_2 a_{2,1} = \frac{1}{2}.\\
\end{cases} 
\eeq
The system \eqref{eq:sys_rk2} is underdetermined: there are 4 unknowns but only 3 equations. Choosing $c_2 \equiv c$ as a free parameter, we obtain
\beq
\label{eq:param_rk2}
a_{2,1} = c\com \qquad b_2 = \frac{1}{2c}\com \qqand b_1 = 1 - \frac{1}{2c}\per
\eeq
The parameter $c$ can be chosen to give the best accuracy. Typically $\tfrac{1}{2}\leq c\leq 1$. The linear stability is independent of $c$. For the linear problem $F = \lambda u$. Thus
\beq
\label{eq:stab_rk2}
u^{n+1} = u^n \left(1 + \lambda\dt + \frac{(\lambda\dt)^2}{2}\right)\per
\eeq
Stability requires
\beq
\label{eq:stab_constraint_rk2}
\sigma = \left|1 + \lambda\dt + \frac{(\lambda\dt)^2}{2} \right| \leq 1\per
\eeq
Along the real axis, the stability the stability constrain \eqref{eq:stab_constraint_rk2} is
\beq
\label{eq:stab_real_rk2}
 1 + \lambda_r \dt + \frac{(\lambda_r\dt)^2}{2} \leq 1\com
\eeq
which gives
\beq
-2 \leq \lambda_r \dt \leq 0\per
\eeq
Along the imaginary axis, the stability region only touches $\lambda_i=0$. Figure \ref{fig:stab_region} shows the linear stability region for he RK2 scheme.


\begin{figure}[ht]
\begin{center}
\includegraphics[width=12pc,angle=0]{figs/stability_region_rk2.png}
\includegraphics[width=12pc,angle=0]{figs/stability_region_rk3.png}
\includegraphics[width=12pc,angle=0]{figs/stability_region_rk4.png}
\end{center}
\caption{Stability region for Runge-Kutta schemes.}
\label{fig:stab_region}
\end{figure}


\subsection{RK4}
Derivation of higher-order RK schemes is straightforward but clumsy. A popular scheme is RK4

\beq
\label{rk4}
u^{n+1} = u^n+ \frac{\dt}{6}(  k_1 + 2 k_2 + 2 k_3 + k_4)\com 
\eeq
where
\begin{align}
k_1 &= F(u^n,t^n ) \com \nonumber \\
k_2 &= F(u^n + \half k_1 \dt, t^n + \half \dt )\com \nonumber \\
k_3 &= F(u^n + \half k_2 \dt, t^n + \half \dt )\com \nonumber \\
k_4 &= F(u^n + k_3 \dt, t^n + \dt )\per
\end{align}
This RK4 is one of my favorite time marching  schemes. It is easy to implement, and has good stability properties (see figure \ref{fig:stab_region}).

\subsection{A low-storage RK3 scheme}

To simply notation we write
\beq
\label{eq:lin_nlin}
\p_t u = \sL(u) + \sH(u)\com
\eeq
where $\sL$ comprises the linear terms and $\sH$ contains the nonlinear terms. The low-storage RKW3 scheme is in the form

\beq
\label{eq:rkw3}
u^{n+1} = u^\dstar + \dt [\sL(\alpha_3 u^\star + \beta_3 u^{n+1})] + \gamma_3 \sH(u^\dstar) + \xi_2 \sH(u^\star)\com
\eeq
where
\beq
\label{eq:rkw3_aux}
u^\star = u^n + \dt [\sL(\alpha_1 u^n + \beta_1 u^\star) + \gamma_1 \sH(u^n)]\com
\eeq
and
\beq
\label{eq:rkw3_aux2}
u^\dstar = u^\star + \dt [\sL(\alpha_2 u^\star + \beta_2 u^\dstar) + \gamma_2 \sH(u^\star) + \xi_1 \sH(u^n)]\com
\eeq
with
\beq
\label{eq:rkw3_gammas}
\gamma_1 = \frac{8}{15}\com\qquad \gamma_2 = \frac{5}{12}\com \qqand \gamma_3 = \frac{3}{4}\com
\eeq
\beq
\label{eq:rkw3_betas}
\beta_1 = \frac{37}{160}\com\qquad \beta_2 = \frac{5}{24}\com\qqand \beta_3 = \frac{1}{6}\com
\eeq
\beq
\label{eq:rkw3_alphas}
\alpha_1 = \frac{29}{96}\com\qquad \alpha_2 = -\frac{3}{40}\com\qqand \alpha_3 = \frac{1}{6}\com
\eeq
and
\beq
\label{eq:rkw3_xis}
\xi_1 = -\frac{17}{60}\com\qqand \xi_2 = -\frac{5}{12}\per
\eeq

%Here we only present the results. A popular RK3 scheme is
%\beq
%\label{rk3}
%u^{n+1} = u^n+ \dt(b_1  k_1 + b_2 k_2 + b_3 k_3)\com 
%\eeq
%where
%\begin{align}
%k_1 &= F(u^n,t^n ) \com \nonumber \\
%k_2 &= F(u^n + a_{2,1} k_1 \dt, t^n +c_2 \dt )\com \nonumber \\
%    k_3 &= F(u^n + a_{2,1} k_1 \dt + a_{3,1} k_1 \dt + a_{3,2} k_2 \dt + , t^n +c_2 \dt + c_3 \dt)\com
%\end{align}
%with
%\beq
%b_1=\frac{1}{4}\com \qquad b_2 = 0\com\qqand b_3 = \frac{3}{4}\com
%\eeq
%and
%\beq
%a_{2,1} = \frac{8}{15} \com\qquad a_{3,1} = \frac{1}{4}\com \qqand a_{3,2} = \frac{5}{12}\com
%\eeq
%and
%\beq
%c_2 = \frac{8}{15} \qqand c_3 = \frac{2}{3}\per
%\eeq

\section{$\theta$-schemes}
For the linear viscous terms, it is customary to use implicit schemes. The $\theta$-schemes avoid rapid oscillations for large $\lambda \dt$, while Crank-Nicolson's second-order. These schemes have the form
\beq
\label{eq:th_schemes}
u^{n+1} = u^n + \dt [(1-\theta)F^n + \theta F^{n+1}]\per
\eeq
Notice that CN  is recovered with $\theta = \tfrac{1}{2}$, and implicit Euler is obtain with $\theta = 1$. Typically $\tfrac{1}{2}\leq\theta\leq 1$. 

\subsection*{Linear stability}
The growth factor is
\beq
\label{eq:lin_stab_th}
\sigma = \frac{1 + (1-\theta)\lambda \dt}{1 - \theta \lambda \dt} \per
\eeq
Thus
\beq
\lim_{\lambda\dt\to-\infty} \sigma = -\frac{1-\theta}{\theta} \leq 1 \per
\eeq
With $\theta = \tfrac{3}{4}$, the limit above is $-\tfrac{1}{3}$, a third of CN's limit. 

\subsection*{Order of accuracy}
From the $\theta$-scheme we have
\beq
\frac{u^{n+1}}{u^n} = 1 + \lambda\dt + \theta \lambda^2 \dt^2 + \mathcal{O}(\lambda^3\dt^3).
\eeq
Comparing with the expansion for the exponential $\ee^{\lambda t}$, we conclude that
\beq
\text{Err}_\theta \sim \left(\frac{1}{2}-\theta\right)(\lambda \dt)^2\per 
\eeq
With $\theta = \tfrac{3}{4}$ the error is
\beq
\text{Err}_{3/4} \sim \frac{1}{4}(\lambda \dt)^2\com
\eeq
which is half of the error of the implicit Euler scheme.

% Spectral methods
\section{Spectral methods}
Spectral methods are the golden choice for numerical methods. In problems with simple geometry we express the solution as a linear combination of basis functions
\beq
\label{eq:lin_comb_basis}
u(x_j,t) = \sum_{n=1}^\nmax \hat{u}_n(t) \sb_n(x_j)\com
\eeq
where $b_n(x)$ is the $n$'th basis function. A few desired properties for the basis are

\begin{enumerate}

        
\item Orthogonality 
    \subitem This property ensures accuracy. The basis formed by the set $\{b_n\}$ usually derives from a Sturm-Liouville problem on the domain of interest. The basis functions are typically orthogonal with respect to an inner product $<>$:
\beq
\label{eq:orthogonality}
<\sb_n\,\sb_m> = \alpha_{mn}\,\delta_{mn}\com
\eeq
where $\delta$ is the Kronecker delta, and $\alpha$ is a normalization constant, usually taken to be 1.

The coefficient $\hat{u}_n$ is then found by projection of the true solution on the $n$'th basis function:
\beq
\label{eq:projec}
\hat{u_p} = <\sb_p\,u(x,t)>\per
\eeq

\item Derivatives are known analytically
    \subitem This property allows easy computation of derivatives, e.g.
    \beq
        \label{eq:deriv_basis}
        \p_x u(x_j,t) = \sum_{n=1}^\nmax \hat{u}_n \p_x \sb_n\per
    \eeq
    We assume that the series for the derivative converges within the domain. This is always the case when $u(x,t)$ is periodic and sufficiently smooth within the domain. If $u(x,t)$ has cornes, discontinuities, etc differentiating term-by-term may result in a nonconvergent series. We will return to this issue later.

\item A computational efficient way to transform form the physical domain to the spectral domain.
    \subitem We should be able to easily and efficiently compute $u$ form $\hat{u}_n$, and vice-versa. An example of transform is
    \beq
        \label{eq:ex_transform}
        \hat{u}_n(t) = \frac{1}{\nmax} \sum_{j=1}^{\nmax-1} u(x_j,t) \sb_n(x_j)\com
    \eeq
    Evaluation of the sum in above requires $\mathcal{O}(\nmax)$ operations for every $n$. Thus the cost of the transform is  $\mathcal{O}(\nmax^2)$. For a spectral method to be efficient we should be able to come up with algorithms that reduce this cost, and that can be implemented in parallel. For instance, the celebrated Fast Fourier Transform (FFT)
    reduces this cost to $\mathcal{O}(\nmax \log_2 \nmax)$. This results in a significant reduction of number of operations for large $\nmax$ (see table \ref{tab:nlogn}). For instance, for $\nmax=256$ this results in a saving of $\sim80\%$! Turbulence simulations would not be possible without the  FFT.

\begin{table}
\label{tab:nlogn}
\caption{A comparison of $\nmax^2$ versus $5\nmax\log_2\nmax$}
\centering
\begin{tabular}{l| c c }
    $\nmax$ & $5 \nmax\log_2\nmax$ & $\nmax$ \\
    \hline
    8 & 120 & 64 \\
    16 & 320 & 256\\
    32 & 800 & 1024\\
    64 & 1920 & 4096\\
    128 & 4480 & 16384\\
    256 & 10240 & 65536\\
\hline
\end{tabular}
\end{table}

\end{enumerate}

\subsection*{Fourier spectral methods}
We will focus on the most used class of methods, namely the Fourier spectral methods. In Fourier methods, the set $\{\sb_n\}$ is formed by complex exponentials $\{\exp(\ii \kappa_n x_j)\}$ where
\beq
\label{eq:eigs_fourier}
\kappa_n \defn \frac{2\pi n}{L}\com\qquad n = -\frac{\nmax}{2}\com-\frac{\nmax}{2}+1\com\ldots\com\frac{\nmax}{2}-2\com-\frac{\nmax}{2}-1\per
\eeq
The inverse transform is defined by a truncated Fourier series
\beq
\label{eq:idft}
u(x_j) = \sum_{n=-\nmax/2}^{\nmax/2-1}\!\!\! \hat{u}_n\,\ee^{\ii \kappa_n x_j}\com
\eeq
where $\hat{u}_n$ is the $n$'th Fourier coefficient, which is generally complex-valued. Notice that the series \eqref{eq:idft} implies that $u$ is periodic with period $L$
\beq
\label{eq:idft}
u\left(-\tfrac{L}{2}\right) = u\left(\tfrac{L}{2}\right)\per
\eeq
The discrete Fourier transform (DFT) is
\beq
\label{eq:dft}
\hat{u}_n = \frac{1}{\nmax} \sum_{j=-\nmax/2}^{\nmax/2-1}\!\! u_j \ee^{-\ii \kappa_n x_j}\per
\eeq
We emphasize that one should sum only up to $\nmax/2-1$ since the $u$ is periodic, so that the data at $x_{\nmax/2}$ add no information to the problem. This definition is consistent with the inverse transform \eqref{eq:idft}, following from the orthogonality of the discrete complex exponentials. This property is fundamental in many calculations, so we shall formally prove it. Consider the summation
\beq
\label{eq:orthogonal_dce0}
\sum_{j=0}^{N-1} \ee^{\ii \left(\frac{2\pi}{\nmax}(n-s)\,j\right)} \com
\eeq
where we used $x_j = \tfrac{L}{\nmax}j$. The summation $S$ is a geometric series 
\beq
s \defn 1 + r + r^2 + \ldots + r^{\nmax-1}\com \qquad \text{with}\qquad r = \ee^{\ii\left(\frac{2\pi}{\nmax}(n-s)\right)}\per
\eeq
Hence
\beq
rs = r + r^2 + \ldots + r^{\nmax} \Longrightarrow s - rs = 1 - r^{\nmax}\com
\eeq
and therefore
\beq
s = \frac{1-r^{\nmax}}{1 - r}\com\qquad r \neq 1\per
\eeq
Now
\beq
r^\nmax = \ee^{\ii 2\pi(n-s)} = 1\com\qquad\text{if}\qquad n \neq s + m\,\nmax\com
\eeq
where $m$ is an integer.
Thus
\beq
\label{eq:orthogonal_dce}
\sum_{j=0}^{N-1} \ee^{\ii \left(\frac{2\pi}{\nmax}(n-s)\,j\right)} =  
\begin{cases}
\nmax:& n = s + m \nmax\com\qquad m=0\pm 1 \pm 2 \pm3\ldots\,;\\
    0:& \text{otherwise}\per
\end{cases}
\eeq


\subsubsection*{Properties of the DFT}
The discrete Fourier pair \eqref{eq:idft}-\eqref{eq:dft} satisfy the desired properties discussed above. This is the reason for the popularity of Fourier methods. The set $\{\exp(\ii \kappa_n x_j)\}$ forms a complete orthogonal basis on any interval of size $L$ (e.g. $[0,L]$, $[-L/2,L/2]$, etc). Moreover, there exists the FFT algorithm performs the DFT very efficiently. In particular, the FFTW implementation in Fortran and C is very popular.  A summary of important properties is

\begin{enumerate}
    \item As mentioned above the complex exponential are orthogonal with respect to the simplest inner product
        \beq
            \label{eq:orthogonality}
            <\ee^{\ii \kappa_n x}\ee^{\ii \kappa_m x }> \defn \frac{1}{L}\int_{0}^{L}\ee^{\ii \kappa_n x}\ee^{\ii \kappa_m x } \dd x = \delta_{mn}\per
        \eeq
    \item The function $u$ is periodic with period $L$
        \beq
            \label{eq:periodicity_phys}
            u(0) =  \sum_{n=-\nmax/2}^{\nmax/2-1}\!\!\! \hat{u}_n\ee^{\ii \kappa_n\times 0} =  \sum_{n=-\nmax/2}^{\nmax/2-1}\!\!\! \hat{u}_n \ee^{\ii \kappa_n \times L} =  u(L) 
        \eeq

    \item The Fourier coefficients are periodic with period $\nmax$
        \beq
        \label{eq:periodicity_spec}
        \hat{u}_{n+\nmax} = \frac{1}{\nmax} \sum_{j=-\nmax/2}^{\nmax/2-1}\!\! u_j \ee^{-\ii\, \frac{2\pi}{L}(n+\nmax) \frac{j\, L}{\nmax}} = \frac{1}{\nmax} \sum_{j=-\nmax/2}^{\nmax/2-1}\!\! u_j \ee^{-\ii \,2\pi \frac{n\,j}{\nmax}} = \hat{u}_n \per
        \eeq


    \item The zeroth mode represents the average of the function. With $\kappa_0 = 0$, we have
        \beq
            \label{eq:zeroth}
            \hat{u}_0 = \frac{1}{\nmax}\sum_{j=-\nmax/2}^{\nmax}\!\! u_j\per
        \eeq
        Note that some implementations of the DFT do not divide by $\nmax$ in the definition \eqref{eq:idft}. This is simply a matter of definition; one must alway check DocStrings and algorithm manuals to make sure which convention in being used.

\end{enumerate}
   
\subsubsection*{Truncation error, aliasing, and other potential problems}
The definition of the inverse DFT assumes that $u$ has nice properties such as continuity of the function and its derivatives. If the function is discontinuous, then Gibbs oscillation might be introduce Gibbs oscillation, which amount for a $10\%$ error near the discontinuities. The rapid Gibss oscillations can be significantly reduced by using filters.   

A potential problem in spectral methods is aliasing of unresolved high-frequency modes. These modes alias back into low-frequency modes. For linear problems this is not an issue provided that the initial conditions do not contain unresolved high-frequency content. For non-linear problems, such as the NS equations, high-frequency modes are introduced every time step.

\subsubsection*{Parseval's theorem}
The Parseval's relation essentially says that the total energy in physical space is equal to the total energy in Fourier space
\beq
\label{eq:parseval}
\sum_{j=0}^{\nmax-1} u_ju_j^* = \nmax \sum_{n=-\nmax/2}^{\nmax/2-1}\!\! |\hat{u}_n|^2\per
\eeq
The proof of this relationship is simple. We introduce the inverse DFT definition \eqref{eq:idft} into \eqref{eq:parseval}
\begin{align}
\label{eq:parseval_1}
\sum_{j=0}^{\nmax-1} u_ju_j^* &= \sum_{j=0}^{\nmax-1} \left( \sum_{n=-\nmax/2}^{\nmax/2-1}\hat{u}_n \ee^{\ii \kappa_n x_j} \right) \left( \sum_{m=-\nmax/2}^{\nmax/2-1}\hat{u}_m^* \ee^{-\ii \kappa_m x_j} \right) \nonumber\\
                              & =\sum_{n=-\nmax/2}^{\nmax/2-1}\sum_{m=-\nmax/2}^{\nmax/2-1}\hat{u}_n \hat{u}_m^*\underbrace{\left(\sum_{j=0}^{\nmax-1} \ee^{\ii(\kappa_n -\kappa_m)x_j}\right)}_{= \nmax \delta_{nm}}\per
\end{align}


\subsubsection*{Truncation error}
To study the truncation error associated with a finite $\nmax$ in \eqref{eq:idft} we consider the true solution
\beq
\label{eq:ift}
u_e = \sum_{n=-\infty}^{\infty}\! \tilde{u}_n \ee^{\ii \kappa_n x_j}\com
\eeq
where the Fourier coefficient is
\beq
\label{eq:ft}
\tilde{u}_n = \frac{1}{L}\int_0^{L}\! u_e(x)\ee^{\ii \kappa_n x}\per
\eeq
Notice that, by aliasing, we have
\beq
\label{eq:uhat_alias}
\hat{u}_n = \tilde{u}_n + \sum_{m=-\infty}^{\infty} \tilde{u}_{n+m\nmax}\per
\eeq
Thus
\beq
\label{eq:idft_2}
u_j = \sum_{n=-\nmax/2}^{\nmax/2-1}\left(\tilde{u}_n + \sum_{m=-\infty}^{\infty}\tilde{u}_{n+m\nmax}\right)\ee^{\ii \kappa_n x_j}\com
\eeq
and the $L_2$-norm of the error is
\beq
\label{eq:error_idft}
\frac{1}{L} \int_0^{L}\left|u_e(x)- \sum_{n=-\nmax/2}^{\nmax/2-1}\!\!\! \hat{u}_n\,\ee^{\ii \kappa_n x_j}\right|^2 \dd x = \underbrace{\sum_{|n|>\nmax/2} |\tilde{u}_n|^2}_{\text{truncation error}} + \underbrace{\sum_{-n=\nmax/2}^{\nmax/2-1}\left| \sum_{m=-\infty}^{\infty}\tilde{u}_{n+m\nmax}\right|^2}_{\text{aliasing error}}\per
\eeq

\subsubsection*{Convergence of the inverse DFT}
Integrating \eqref{eq:ft} by parts we obtain
\begin{align}
\label{eq:error_trunc}
\tilde{u}_n =\frac{1}{L}\int_0^L\! u_e(x) \ee^{\ii \kappa_n x} dx &= -\frac{1}{L}\int_0^L  \frac{u_e'(x) \ee^{\ii\kappa_n }}{\ii \kappa_n} \dd x \nonumber \\ &  = \frac{1}{L} \int_0^L \frac{u_e''(x) \ee^{\ii \kappa_n x}}{(\ii \kappa_n)^2} \dd x\per
\end{align}
We can continue the process above as long as the derivate are smooth and periodic. Hence
\beq
\label{eq:trunc_error_bound}
|\tilde{u}_n|^2 \lesssim (\Delta x)^{-2p}\com
\eeq
for all $p \in \mathbb{N}$. Thus for smooth functions in physical space, spectral methods beats the most accurate finite differences scheme. Spectral accuracy is also know as exponential accuracy. A rule of thumb is that for a given data spacing $\Delta x$, a spectral scheme with $3\Delta x$ does a better job than second-order finite-differences scheme.

\subsubsection*{Gibbs phenomenon}
Consider the step function
\beq
\label{eq:step}
S_e(x) = \begin{cases}
            1\com & 0\leq x < \pi\,;\\
0\com & \pi \leq x < 2 \pi\per
         \end{cases}
\eeq
A truncated Fourier series representation is
\beq
\label{eq:step_trunc}
S_{T}(x) = \sum_{n=-\nmax/2}^{\nmax/2} \!\!\tilde{S}_n \ee^{-\ii n x}\com
\eeq
where the Fourier coefficients are
\beq
\label{eq:step_hat}
\tilde{S}_n = \frac{1}{2\pi} \int_0^\pi\!\! S_e(x) \ee^{\ii n x} \dd x\per
\eeq
Thus
\begin{align}
\label{eq:step_trunc_2}
S_T(x) =  \sum_{n=-\nmax/2}^{\nmax/2} \!\!\left( \frac{1}{2\pi} \int_0^\pi\!\! S_e(\xi) \ee^{-\ii n \xi}
\dd \xi \right) \ee^{\ii n x} = \frac{1}{2\pi} \int_0^\pi \underbrace{S_e(\xi)}_{= 1}
        \underbrace{\left(\sum_{n=-\nmax/2}^{\nmax/2} \ee^{\ii n (x-\xi)} \right)}_{\defn\sDN} \dd \xi\com
\end{align}
where $\sDN$ is the Dirichlet Kernel
\beq
\label{eq:dk_defn}
\sDN(z) = \begin{cases}
    \sin\left({\nmax+1\over 2} z\right):& \text{if} \qquad z \neq 2\pi j\,; \\
    \nmax + 1:&\text{if}  \qquad z = 2\pi j\per
        \end{cases}
\eeq
Figure \ref{fig:dk} shows the Dirichlet kernel for truncation indices $\nmax$ from 1 through 11. As $\nmax$ increases the kernel distributionally approaches a delta function at $z = 0$.


\begin{figure}[ht]
\begin{center}
    \includegraphics[width=20pc,angle=0]{figs/dirichlet_kernel}
\end{center}
\caption{The Dirichlet kernel \eqref{eq:dk_defn} for various truncation indices $\nmax$.}
\label{fig:dk}
\end{figure}

We can rewrite the truncated series  \eqref{eq:step_trunc_2} by changing variables with $y\defn x - \xi$
\beq
S_T(x) = \frac{1}{2\pi} \int_{x-\pi}^x \sDN(y) \dd y = \frac{1}{2\pi} \left[ \int_0^x \sDN(y) \dd y
+ \underbrace{\int_{-\pi}^0 \sDN(y)\dd y}_{\approx \pi} 
+ \underbrace{\int_{x-\pi}^{\pi}\sDN(y)\dd y}_{<< 1} \right]   \per
\eeq
We have
\beq
\int_0^x \sDN(y) \dd y \approx 1.08949 \pi\com\qquad \text{for} \qquad x \qquad \text{small}\per
\eeq
So that in the vicinity of the discontinuity at $x=0$ we have
\beq
S_T(x) \approx 1.09\per
\eeq
That is, the truncated series $S_T$ overshoots the exact value by about $9\%$. This phenomenon is the Gibbs oscillation.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=20pc,angle=0]{figs/step_function}
\end{center}
\caption{The representation of a step function by Fourier series under various truncation. Notice the Gibbs phenomenon near the discontinuity.}
\label{fig:step}
\end{figure}


\subsection*{Filters}
To remove undesired Gibbs oscillations that may arise from discontinuities present in the initial conditions or developed by the equation (e.g., Burger's equation) we can use filters. The filtering process can operate in two levels
\begin{enumerate}
    \item Level of equation
        \subitem We introduce an additional term, e.g., artificial viscosity or hyperviscosity, that will filter out highwanumber variability. For instance, in the one-dimensional Burger equation, we can add a fourth-order hyperviscosity
        \beq
            \label{eq:burgers_1d_hv}
            \p_t u + u\p_x u = D \p^4_{x^4} u\per
        \eeq
        In Fourier space the solution to the linear hyperviscouss term is $\exp(-k^4 D t)$, which makes clear its low-pass character.

    \item Level of solution
        \subitem Once the solution is obtained, one can filter out highwavenumber oscillations using selective filters. A celebrated example is the use of the Lanczos $\sigma$-factos. In this approach, one substitute the original Fourier series by
        \beq
            \label{eq:lanczos_fact}
            u_j = \sum_{n=-N/2}^{N/2-1} \sigma_n \hat{u}_n \ee^{\ii k_n x_j}\com
        \eeq    
        where $\sigma_n$ is a filter coefficient in the form
        \beq
            \label{eq_sigma}
            \sigma_n = \sigma(\theta)\com\qquad \theta\defn \frac{2\pi n}{\nmax}\per
        \eeq
        The factor $\sigma(\theta)$ must satisfy the properties
        \begin{enumerate}
            \item Reality;
            \item Symmetry about $\theta = 0$; 
            \item $p-1$ continuous differentiable  for $p\ge 1$;
            \item 
                \subitem $\sigma(\theta) = 0$, if $|\theta|\ge \pi$;
                \subitem $\sigma(0) = 1$  (do not change the average);;
                \subitem $\sigma^{(j)} = 0$, for $1\le\j\le p-1$.
        \end{enumerate}

    Some examples of filter in the level of equation include
    \begin{itemize}
        \item Lanczos filter
            \subitem $\sigma(0) = 1$;
            \subitem $\sigma(\theta) = \frac{\sin \theta}{\theta}$.
              \item Raised cosine
            \subitem $\sigma(\theta) = (1+\cos\theta)/2$.
        \item Sharpened raised cosine
            \subitem $\sigma(\theta) = \sigma_0^4(\theta)\left[35-84\sigma_0(\theta
                    )+70\sigma_0(\theta)^2 - 20\sigma_0(\theta)^2\right]$,
            \subitem where $\sigma_0$ is the raised cosine filter.
        \item Exponential filter of order $p$ (generalized viscosity)
            \subitem $\sigma(\theta)=\ee^{-\alpha\theta ^p}$,
            \subitem where $\alpha$ is an arbitrary parameter.
    \end{itemize}
    Figure \ref{fig:step_filters} shows the step function truncated series with $\nmax=9$, and various filtered solutions.

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=20pc,angle=0]{figs/step_function_filters}
\end{center}
\caption{The representation of a step function by Fourier series with $\nmax=9$ and various filtered solutions.}
\label{fig:step_filters}
\end{figure}

\subsubsection*{Aliasing}
The calculation of Fourier transform of nonlinear terms of terms with nonconstant coefficients is very expensive. Consider the two variables 
\beq
u_e(x) = \sum_{-\nmax/2}^{\nmax/2}\tilde{u}_n \ee^{\ii n x}\com\qqand
v_e(x) = \sum_{-\nmax/2}^{\nmax/2}\tilde{v}_n \ee^{\ii n x}\com
\eeq
and their product
\beq
\label{eq:product}
w_e(x) = u_e(x)v_e(x) = \sum_{-\nmax/2}^{\nmax/2}\sum_{-\nmax/2}^{\nmax/2}\tilde{u}_n\tilde{v}_m \ee^{\ii (n+m) x}\com\qqand
\eeq
Hence
\beq
\tilde{w}_k = \frac{1}{2\pi}\int_{\pi}^{\pi} \sum_{-\nmax/2}^{\nmax/2}\sum_{-\nmax/2}^{\nmax/2}\tilde{u}_n\tilde{v}_m \ee^{\ii (n+m-k) x} \dd x
=  \sum_{-\nmax/2}^{\nmax/2}\sum_{-\nmax/2}^{\nmax/2} \tilde{u}_n\tilde{v}_m \underbrace{\frac{1}{2\pi}\int_{\pi}^{\pi}  \ee^{\ii (n+m-k) x}\dd x}_
{=\begin{cases}
        1:& \text{if} \qquad m+n = k;\\
        0:& \text{otherwise}.
 \end{cases}
    }
\eeq
Thus for series with $\nmax$ truncation index, the multiplication in physical space is equivalent to convolution in Fourier space. Computation of convolution sums requires $\mathcal{O}(\nmax^2)$ operations. The alternative is to use the so-called pseudo-spectral methods that quires $\mathcal{O}(\nmax\log_2\nmax)$. In pseudo-spectral methods we compute the products in physical space, and then transform the products to Fourier space to march the system forward, then transform the variables back to physical space to compute the products, and so on.  The problem with this approach is that is introduces aliasing. To see that, note that the discrete Fourier transform   of \eqref{eq:product} is
\begin{align}
\label{eq:dft_wk}
\hat{w}_k =& \frac{1}{\nmax}\sum_{j=-\nmax/2}^{\nmax/2-1}\left(\sum_{n=-\nmax/2}^{\nmax/2-1}
\sum_{m=-\nmax/2}^{\nmax/2-1}\hat{u}_n\hat{v}_m \ee^{\ii (m+n)x_j}\right)\ee^{-\ii k x_j}\nonumber \\
= & \sum_{n=-\nmax/2}^{\nmax/2-1}
\sum_{m=-\nmax/2}^{\nmax/2-1}\hat{u}_n\hat{v}_m  {\frac{1}{\nmax}\sum_{j=-\nmax/2}^{\nmax/2-1}\ee^{\ii (m+n-k)x_j}}\per
\end{align}
Now recall that
\beq
 {\frac{1}{\nmax}\sum_{j=-\nmax/2}^{\nmax/2-1}\ee^{\ii (m+n-k)x_j}}
=\begin{cases}
        1:& \text{if} \qquad m+n = k + pN\com\, p=0,1,2,\ldots;\\
        0:& \text{otherwise}.
 \end{cases}
\eeq
Thus
\beq
\label{eq:alias}
\hat{w}_k =    \sum_{n=-\nmax/2}^{\nmax/2-1}\sum_{m=-\nmax/2}^{\nmax/2-1}\hat{u}_n\hat{v}_m +  \sum\!\!\!\!\!\!\!\!\!\sum_{m+n=k\pm \nmax}\!\!\!\!\!\!\hat{u}_n\hat{v}_m  + \sum\!\!\!\!\!\!\!\!\!\!\sum_{m+n=k\pm 2\nmax}\!\!\!\!\!\hat{u}_n\hat{v}_m \ldots\per
\eeq
The first term on the right of \eqref{eq:alias} is the discrete Fourier transform  of the product $u v$. The double sums represent aliasing terms. These terms are due to unresolved high frequency modes that project back into the lower frequency modes. 
\end{enumerate}
For quadratic nonlinearity we only need to consider the first term on the right of \eqref{eq:alias}. The extrema alias frequencies are
\beq
(m+n)_{max} = \nmax-2\com\qqand (m+n)_{min} = -\nmax\per
\eeq
Thus we only need to consider frequencies from $-3\nmax/2$ through $3\nmax/2$. 

\subsection*{Dealiasing}
There are two main techniques used to remove the aliased frequencies due to nonlinearities. 

\subsubsection*{Zero padding}
We use discrete Fourier series for $u$, $v$, and $w$ with $\mathrm{M}\ge 3\nmax/2$, and set $\hat{u}_n = \hat{v}_m = 0$ for $m\ge\nmax/2$ before going to physical domain. We also set $\hat{w}_n$ for $n\ge \nmax/2$ 
 after going back to Fourier space.

Notice that the worst case scenario is $n=m=-\nmax/2$:
\beq
-\frac{\nmax}{2} -\frac{\nmax}{2} = \frac{\nmax}{2} \pm \mathrm{M} \Rightarrow \mathrm{M} = \frac{3}{2}\nmax\per
\eeq
The computational cost of zero padding is $\mathcal{O}(3/2)^n$, where $n$ is the number of dimensions. Thus zero padding can be quite expensive, particularly in two- or three-dimensions. 

\subsubsection*{Phase shift}
Consider the product of $u$ and $v$ computed at the original grid
\beq
\label{eq:prod_orig_grid}
w(x) = u(x)v(x)\com
\eeq
and the products computed in a grid shifted by $\xi$
\beq
\label{eq:prod_shifted_grid}
w(x+\Delta x) = u(x+\xi)v(x+\xi)\per
\eeq
The DFT of \eqref{eq:prod_orig_grid} is
\beq
\label{eq:alias1}
\hat{w}_{o,k} =    \sum_{n=-\nmax/2}^{\nmax/2-1}\sum_{m=-\nmax/2}^{\nmax/2-1}\hat{u}_n\hat{v}_m +  \sum\!\!\!\!\!\!\!\!\!\sum_{m+n=k\pm \nmax}\!\!\!\!\!\!\hat{u}_n\hat{v}_m\com
\eeq
whereas the DFT of \eqref{eq:prod_shifted_grid} is
\beq
\label{eq:alias2}
\hat{w}_{\xi,k} =    \sum_{n=-\nmax/2}^{\nmax/2-1}\sum_{m=-\nmax/2}^{\nmax/2-1}\hat{u}_n\hat{v}_m + \ee^{\pm\ii 
\nmax \xi} \sum\!\!\!\!\!\!\!\!\!\sum_{m+n=k\pm \nmax}\!\!\!\!\!\!\hat{u}_n\hat{v}_m\per
\eeq
The idea is to combine \eqref{eq:alias1} and \eqref{eq:alias2} in such a way as to eliminate the aliasing terms
\beq
\hat{w}_{k} = \frac{\hat{w}_{o,k} + \hat{w}_{\xi,k}}{2} =  
\hat{w}_{\xi,k} =    \sum_{n=-\nmax/2}^{\nmax/2-1}\sum_{m=-\nmax/2}^{\nmax/2-1}\hat{u}_n\hat{v}_m + \frac{1+ \ee^{\pm\ii \nmax \xi}}{2} \sum\!\!\!\!\!\!\!\!\!\sum_{m+n=k\pm \nmax}\!\!\!\!\!\!\hat{u}_n\hat{v}_m\per
\eeq
Hence 
\beq
\ee^{\pm\ii \nmax \xi} = -1 \Rightarrow \xi\nmax = \pm \pi  \Rightarrow \xi = \frac{\pi}{\nmax}
= \frac{\Delta x}{2}\per
\eeq
Thus the algorithm for phase shifting is
\begin{enumerate}
    \item Perform the pseudo-spectral without shift;
    \item Shift the Fourier coefficients $\hat{u}_k$ and $\hat{v}_k$ in Fourier space by multiplying by
            $\exp(\ii \nmax \Delta x /2)$. Then perform the pseudo-spectral calculation for this shifted 
            coefficients;
    \item Take the average of (1) and (2), and march the system forward.
\end{enumerate}
    The cost of the phase shifting is $\nmax \log_2 \nmax$, and therefore keeps the same order of operations as the aliased calculation. It may be advantageous if $\nmax$ is too large, since is does not require more memory than the original problem. 

\subsubsection*{Zero padding vs. phase shifting in one dimension}
It is informative to compare the relative cost of the two dealiasing methods described above. Suppose the original grid has $\nmax$ elements. Hence zero padding requires computation of DFT on $\mmax = \tfrac{3}{2}\nmax$ grids. Thus one cycle of the pseudo-spectral calculation requires $15 \mmax \log_2\mmax = \tfrac{45}{2}\nmax\log_2 \nmax$ operations. The phase-shifting dealiasing method requires DFT computations on $\nmax$ grid but the pseudo-spectral cycle must be repeated twice (in one dimension). Thus the total number operations required for dealiasing using phase shifting is  $30 \nmax \log_2 \nmax$. Thus in one dimension phase shifting requires $\tfrac{12}{5}$ more operations than zero padding. The latter, however, requires more memory. Thus, the question of the relative efficiency of these two methods may depend on the computer one wants to perform the calculations.

\subsubsection*{Dealiasing in higher dimensions}
Real world problems are typically two- of three-dimensional. There is a qualitative difference between aliasing in one and two dimensions because for dimensions larger than one various combinations between wavenumbers produce aliasing. For instance if the original signal is
\beq
\label{eq:orig_signal}
s = \sin x + \cos y\com
\eeq
then a quadratic product produces the terms
\beq
\label{eq:aliased_signal1}
\sin^2 x = \frac{1-\cos 2 x}{2}\com
\eeq
\beq
\label{eq:aliased_signal2}
\cos^2 y = \frac{1+\cos 2 y}{2}\com
\eeq
and also the cross-terms
\beq
\label{eq:aliased_signal3}
2 \sin x \cos y = \sin 2 x\per
\eeq
Thus aliasing in higher dimensions is richer.

Consider the representation of the discrete Fourier transform of $\hat{u}_{\vec{n}}$ 
\beq
\label{eq:alias_error_2d_1}
\hat{u}_{\vec{n}} = \sum_{j_1 = 0}^{\nmax_1-1} \sum_{j_2 = 0}^{\nmax_2-1} u_{j_1,j_2} \exp -\ii[ 
k_{n_1}x_{j_1} + k_{n_2}x_{j_2}] \equiv \sum_{\vec{j}=0}^{\vec{N}-1} u_{\vec{j}}\,\,\ee^{\ii k_{\vec{n}\cdot 
\vec{x}_{\vec{j}}}}\com
\eeq
where
\beq
\vec{n} = \left(n_1\com n_2\right)\com \qquad \vec{j} = \left(j_1\com j_2\right)\com\qqand 
\vec{x} = x_{j_1}\vec{e}_1 + x_{j_2}\vec{e}_2\com
\eeq
with $\vec{e}_1$ and $\vec{e}_2$ the unit vector of the two-dimensional Cartesian grid.

Analogously to the one-dimensional case, the aliasing error is obtained by analyzing the DFT of the product of two variables $u$ and $v$. We obtain
\beq
\label{eq:alias_error_2d_2}
\sum_{\vec{m}+\vec{n}=\vec{p}}\!\!\! \tilde{u}_{\vec{m}} \tilde{v}_{\vec{n}} = \underbrace{\sum_{\vec{m}+\vec{n}=
\vec{p}} \!\!\! \hat{u}_{\vec{m}}\hat{v}_{\vec{n}}}_{\text{Truncated series}} \,+\,   \underbrace{\sum_{\vec{m}+\vec{n}=\vec{p}\pm N_1\vec{e}_1}\!\!\!\!\!\!\!\!\!\hat{u}_{\vec{m}}\hat{v}_{\vec{n}} \,+\,
\sum_{\vec{m}+\vec{n}=\vec{p}\pm N_2\vec{e}_2}\!\!\!\!\!\!\!\!\!\hat{u}_{\vec{m}}\hat{v}_{\vec{n}}}_{\text{Single alias errors}} +
\underbrace{\sum_{\vec{m}+\vec{n}=\vec{p}\pm N_1\vec{e}_1\pm N_2\vec{e}_2}\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \hat{u}_{\vec{m}}\hat{v}_{\vec{n}}}_{\text{Double alias error}}\per
\eeq
Table \ref{tab:alias_123d} shows the number of aliasing term for pseudo-spectral problems in 1 through 3 dimensions.

\begin{table}
\label{tab:alias_123s}
\caption{Number of aliasing terms for quadratic nonlinearities in pseudo-spectral methods from 1 through three dimensions.}
\centering
\begin{tabular}{l| c c c}
    & 1D & 2D & 3D\\
    \hline
    single & 1 & 2 & 3\\
    double & 0 & 1 & 3\\
    triple & 0 & 0 & 1\\
\hline
\end{tabular}
\end{table}

\subsubsection*{Zero padding in two dimensions}
The zero padding approach discussed above in one dimension easily generalizes to higher dimensions. That is, we simply apply the 3/2 rule to each dimension. The memory cost increase is $\left(\tfrac{3}{2}\right)^p$ where $p$ is the number of dimensions. Thus in higher dimensions zero padding may significantly slow down the code. 

\subsubsection*{Phase shifting in two dimensions}
Phase shifting becomes trickier in higher dimensions. While phase shifting in one direction,
it introduces error in other directions. Thus a combination of phase shifts are necessary to cancel all errors. 
Table \ref{tab:phase_shift_error_2d} shows the combination of shifts that zeros out the aliasing terms. In two dimensions, phase shifts requires four evaluations of the pseudo-spectral cycle.
\begin{table}
\label{tab:phase_shift_error_2d}
\caption{Phase shift operations to zero out aliasing terms.}
\centering
\begin{tabular}{l| c | c | c  | c}
    $\xi_1$  & $0$ & $\Delta x/2$ & $0$ & $\Delta x/2$\\ 
    $\xi_2$  & $0$ & $0$ & $\Delta y/2$ & $\Delta y/2$\\ 
    \hline
    AE$_{10}$ & 1 & -1 & 1 & -1\\
    AE$_{01}$ & 1 & 1 & -1 & -1\\
    AE$_{11}$ & 1 & -1 & -1 & 1\\
\end{tabular}
\end{table}

\subsubsection*{Hybrid dealiasing schemes}
The idea of the hybrid dealiasing method is to combine a polyhedral truncation with phase shift. The polyhedral truncation is an incomplete zero padding scheme. In particular, one realizes that the corner of the wavenumber domain are insignificant when it comes to studying flows with isotropic statistics. Thus only $|k_{\nmax_1} \pm k_{\nmax_2}| <
\tfrac{3}{2}$ is kept; the other wavenumbers are zeroed out. Similarly, in three dimensions we use the spherical truncation $|k_{\nmax_1}^2 + k_{\nmax_2}^2 + k_{\nmax_3}^2| < \tfrac{4}{9} \nmax^2$. This truncation removes double and triple aliasing terms (and somewhat more mores). After this truncation, one only needs to three evaluations of the pseudo-spectral cycle: one in the original grid and two with shifts in each direction to cancel out the single aliasing errors. Similarly, only 4 evaluations are needed in three dimensions. 

\subsection*{Some practical aspects of FFT routines in Fortran}
We use FFT in the West (FFTW; for details see {www.fftw.org/documentation}). The application of the one dimensional transform is straight forward and follows a common intuition. In higher dimensions, however, the output is given in nonstandard form. In particular, we note that, for a real signal we have
\begin{align}
    \text{2D} \rightarrow \text{CFT}\left[\text{CRFT}(u)\right]\,; \nonumber \\
    \text{3D} \rightarrow \text{CFT}\left\{\text{CFT}\left[\text{CRFT}(u)\right]\right\}\com 
\end{align}
where CFT represents the complex Fourier transform and CRFT represents the complex real Fourier transform. The latter
 takes advantage of the Hermitian symmetric of the Fourier coefficients to perform the calculation faster and requires
  less storage.

    The are two ways of calling the FFTW routine:
    \begin{itemize}
        \item {\bf Out-of-place}
            \subitem The input array is not destroyed. That is, the routine returns an output array
                        different than the input array. For a one dimensional real input with $\nmax$ points, the
                        output has $\tfrac{\nmax+1}{2}$ non-redundant elements.
        \item {\bf In-place}
            \subitem The routine overwrites the input with the output, using the same memory allocation.
                Because the input is real, care has to be taken to interpret the output. In one dimension,
                 if the input goes is distributed as  $0, 1, \ldots, \nmax-2, \nmax-1$. Then the output is 
                 R$_0$, R$_1$, $\ldots$, R$_{N-1}$, I$_{N/2-1}$, $\ldots$, I$_2$, I$_1$\com where R stands
                  for the real part and I for the imaginary part of the Fourier coefficients.
    \end{itemize}

    Things get more complicated in higher dimensions. So, please consult the manual for an accurate description.


\section{Working with incompressible equations}

The incompressible Navier-Stokes equations are
\beq
\label{eq:ns_2}
\p_t \vec{u} + \vec{u}\cdot \nabla \vec{u} = -\frac{\nabla p}{\rho} + \nu \lap \vec{u}\com
\eeq
and
\beq
\label{eq:cons_mass_2}
\nabla\cdot\vec{u} = 0\per
\eeq


\subsection{Fractional step method}
In this approach, we split every time integration step in two substeps. In the first substep, we perfom
 a preliminary estimation of the velocity $bu^*$ neglecting the pressure term altogether. Using an implicit 
  Euler scheme for the linear terms and explicit Euler for the nonlinear terms, we obtain
 
\beq
\label{eq:first_step}
\frac{\bu^* - \bu^n}{\dt} - \nu \lap \bu^*   = \bN(\bu^n)\com\qquad \text{on the volume} \qquad \Omega\com
\eeq
with non slip boundary conditions
\beq
\label{eq:first_bc}
\bu^*=0\com\qquad \text{on the surface} \qquad  \partial \Omega\per
\eeq
Notice that these boundary conditions are necessary in this substeps because it contains the higher derivative
 viscous terms. The second substep, the projection step, deals with the pressure term
\beq
\label{eq:second_step}
\frac{\bu^{n+1} - \bu^*}{\dt} + \nabla p^{n+1} = 0\com
\eeq
where $p$ is the dynamic pressure, and imposes the nondivergence constraint
\beq
    \label{eq:second_nd}
    \nabla\cdot\bu^{n+1}= 0\com
\eeq
and no-normal flow boundary conditions
\beq
\label{eq:second_bc}
\bu^{n+1}\cdot \hat{\bf n}\com \qquad \text{on the surface}  \qquad \p \Omega\per
\eeq
Taking the divergence of the second equation \eqref{eq:second_step} we obtain an elliptic problem for the pressure
\beq
\label{eq:elliptic_pressure}
\lap p^{n+1} = \frac{\nabla\cdot \bu^{*}}{\dt}\com
\eeq
Using the divergence theorem, we obtain
\beq
\frac{\bu^{n+1}\cdot\hat{\bf n}}{\dt} + \frac{\p \phi}{\p {\bf n}} = 0\com \qquad \text{on the surface}  \qquad \p \Omega\per
\eeq
That is, we impose no-slip on $\bu^*$ and no-normal flow on $\bu^*$ and $\bu^{n+1}$. There is an $\mathcal{O}(\dt)$ error in the no-slip boundary condition on $\bu^{n+1}$. The final single time-step  marching scheme has the form
\beq
\label{eq:full_step}
\bu^{n+1} = \bu^* - \nabla p^{n+1} \dt\per
\eeq

\subsection{Pressure correction methods}
This scheme is also based on two substeps. In the first substep we solve the problem implicitly for pressure
\beq
\frac{\bu^*-\bu^n}{\dt} -\nu \lap \bu^* = \bN(\bu^n) - \nabla p^n\com
\eeq
with no-slip boundary conditions
\beq
\bu^* = 0\com \qquad \text{on the surface} \qquad \p \Omega\per
\eeq
The second substep then makes a pressure correction
\beq
\frac{\bu^{n+1}-\bu^*}{\dt} + \nabla\cdot\phi = 0\com
\eeq
by imposing the nondivengece constraint on $\bu^{n+1}$, which implies
\beq
\lap \phi = \frac{\nabla\cdot\bu^*}{\dt}\com\qquad  \text{on the volume} \qquad \Omega\com
\eeq
subject to no-slip boundary conditions i.e., 
\beq
\frac{\p \phi}{\p n} = 0 \com\qquad  \text{on the surface} \qquad \p \Omega\per
\eeq
Thus the corrected pressure is
\beq
p^{n+1} = p^n + \phi -\nu \nabla\cdot\bu^*\per
\eeq
For this scheme, the no-slip boundary conditions are affected by a $\dt^2$ error term. 

\section{Pressure correction with RKW3}
Following the notation used above for the RKW3 scheme, the first substep of the pressure correction
 method has the form
 \beq
 \frac{\bu^*-\bu^n}{\dt} = -\gamma_1 \sH^n + \frac{\alpha_1}{\Re}\sL \bu^* -(\alpha_1+\beta_1)\sG p^n\com
 \eeq
 \beq
    \frac{\bu^2-\bu^*}{\dt} = -(\alpha_1\beta_1)\sG \phi\com
 \eeq
 \beq
    \sD\cdot\bu^* = 0 \Longrightarrow \sL\phi^1 = \frac{\sD\bu^*}{(\alpha_1+\beta_1)\dt}\com\qquad
     \text{on the volume}\qquad \Omega\com
 \eeq
    and therefore
\beq
\p_n\phi' = 0\com\qquad \text{on the surface}\qquad \p \Omega\per
\eeq
The pressure correction has the form
\beq
p^2 = p^n + \phi^1 - \frac{1}{\Re} \frac{\beta_1}{\alpha_1+\beta_1}\sD\cdot\bu^*\per
\eeq

The second substep is
\beq
\frac{\bu^{**}-\bu^2}{\dt} - \gamma_2 \sH^2 -\xi_2\sH^n + \frac{\alpha_2}{\Re}\sL \bu^2 + 
    \frac{\beta_2}{\Re} \sL \bu^** - (\alpha_2 + \beta_2) \sG p^2\com
\eeq
\beq
\frac{\bu^3-\bu^{**}}{\dt} = -(\alpha_2+\beta_2) \sG p^2\com
\eeq
\beq
\sD\cdot\bu^3 = 0 \Longrightarrow \sL\phi^2 = \frac{\sD u^{**}}{(\alpha_2+\beta_2)\dt}\per
\eeq
The pressure correction in this substep is
\beq
p^3 = p^2 + \phi^2 -\frac{\beta_2}{(\alpha_2+\beta_2)\Re}\sD\cdot\bu^{**}\per
\eeq
The third substep is
\beq
\frac{\bu^{***}-\bu^3}{\dt} - \gamma_3 \sH^3 -\xi_3\sH^2  + \frac{\alpha_3}{\Re}\sL \bu^3 + 
\frac{\beta_3}{\Re} \sL \bu^{***} - (\alpha_3 + \beta_3) \sG p^3\com
\eeq
\beq
\frac{\bu^{n+1}-\bu^{***}}{\dt} = -(\alpha_3+\beta_3) \sG \phi^3\com
\eeq
\beq
\sD\cdot\bu^3 = 0 \Longrightarrow \sL\phi^3 = \frac{\sD u^{***}}{(\alpha_3+\beta_3)\dt}\per
\eeq
Finally, the pressure at step $n+1$ is
\beq
p^{n+1} = p^3 + \phi^3 -\frac{1}{\Re} \frac{\beta_3}{\alpha_3+\beta_3}\sD\bu^{***}\per
\eeq

\section{Solving large algebraic systems}
As we have seen, working with incompressible Navier-Stokes equations give rise to a Poisson 
 problem to be solve every time step. The discrete version of this problem is a large  system
  of algebraic equations, whose solution is computationally costly. In this section, we explore
  some of the main iterative approaches to solve such systems.

\subsection{Relaxation}
    The problem to solve is
    \beq
        \label{eq:poisson}
        \lap \phi = f\com
    \eeq
    where f is a given function of space. In the relaxation method, we add time dependence to
     the problem
    \beq
        \label{eq:poisson_time}
        \p_t \phi = \lap \phi - f\com
    \eeq
    and look for the steady solution. Because we are not interested in the transients, we simply
    step this system forward using an implicit scheme, given an initial guess. Because the true
     solution is steady, this is equivalent of solving for the residual due to the initial guess
     \beq
        \p_t r = \lap r\per
     \eeq
    In Fourier space we have
    \beq
        \hat{r}_k = \hat{r} \ee^{-k^2 t}\per
    \eeq
    Clearly, high wavenumbers $k$ converge faster that low wavenumbers, since the residual of the
     former decays faster.

\subsection{Alternate direction implicit}
For two-dimensional or three-dimensional problems, we generally simplify the problem by making an
 operator split
 \beq
    \label{eq:operator_split}
    [\sI - \dt (\sL_x + \sL_y)] = [\sI - \dt\sL_x ][\sI - \dt\sL_y] + 2 \sL_x \sL_y \dt^2 \com 
 \eeq
    where $\sL_x$ and $\sL_y$ are the discrete Laplacian operator in each direction. The split above
     is accurate to second-order in time, which is the accuracy of a Crank-Nicolson scheme. We then
      solve the problem in two steps
      \beq
            [\sI-\dt\sL_x] r^* = r^n\com
      \eeq
      and
      \beq
      [\sI-\dt\sL_y] r^{n+1} = r^*\per
      \eeq
    These two step are much easier to solve than the original problem because they're series of 
     tri-diagonal systems. This ADI scheme can be embedded in the Crank-Nicolson scheme
     \begin{align}
         [\sI - \tfrac{\dt}{2}\sL_x] r^* =& [\sI + \tfrac{\dt}{2}\sL_y] r^n\\
         [\sI - \tfrac{\dt}{2}\sL_y] r^{n+1} =& [\sI + \tfrac{\dt}{2}\sL_x] r^*\per
    \end{align}
    Notice that the growth factor at each sub-steps is
    \beq
        \sigma_k = \frac{1-k^2\tfrac{\dt}{2}}{1 + k^2\tfrac{\dt}{2}}\per
    \eeq
    Figure \eqref{fig:gf_adi} shows the absolute values of the growth factor $|\sigma_k|$ as a function
     of time step for various values of $k \dx$. As can be seen, different wavenumbers have distinct 
      optimum time steps. The optimum time step for low wavenumbers is typically very large; the contrary
       is true for very small wavenumbers:

       \begin{align}
            \delta_{max} = \dt \left(\frac{\pi}{\dx}\right)^2 = \dt k_{max}^2\com\\
            \delta_{min} = \dt \left(\frac{\pi}{\dx}\right)^2 = \dt k_{min}^2=\frac{\delta_{max}}{\nmax^2}\com
        \end{align}

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=20pc,angle=0]{figs/growth_factor_adi_cn.png}
\end{center}
\caption{The growth factor of the ADI-CN method for various values of $k \dx$.}
\label{fig:gf_adi}
\end{figure}

(SOME SUTFF AND DON'T UNDERSTAND WHERE IT COMES FROM. FOR INSTANCE HOW TO OBTAIN THIS DELTA -8, ETC)

The optimum $\dt$ is then
\beq
\dt_{\text{opt}} = 2\frac{\dx^2}{\pi^2} \nmax = 2\frac{L_x \Delta x}{\pi^2}\com
\eeq
which is the geometric mean between the extrema.  An estimative of the number of iterations
 required for convergence within machine precision is
 \beq
        \left|\frac{r^{n+1}-r^n}{r^0}\right| = \text{Rel. err.}\per
 \eeq
    Thus
    \beq
    \log \text{Rel. err.} = \log \text{tol.} = n \log(\epsilon-1)\com
    \eeq
    where
    \beq
        \epsilon = 1 - 2 \frac{\delta_{max}}{\nmax^2} = 1 - \frac{4}{\nmax}\per
    \eeq

    We can always sweep randomly through the $\dt$ space. In this case, it can be shown that we obtain a faster convergence ($\nmax^{1/2}$).

\end{document}


