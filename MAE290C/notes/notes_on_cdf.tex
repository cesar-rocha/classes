
\documentclass[11pt]{article}

%% WRY has commented out some unused packages %%
%% If needed, activate these by uncommenting
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
%\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
\geometry{a4paper,left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
%\geometry{landscape}                % Activate for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent

%for figures
%\usepackage{graphicx}

\usepackage{color}
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}
%% for graphics this one is also OK:
\usepackage{epsfig}

%% AMS mathsymbols are enabled with
\usepackage{amssymb,amsmath}

%% more options in enumerate
\usepackage{enumerate}
\usepackage{enumitem}

%% insert code
\usepackage{listings}

\usepackage[utf8]{inputenc}

\usepackage{hyperref}

%% colors
\usepackage{graphicx,xcolor,lipsum}


\usepackage{mathtools}

\usepackage{graphicx}
\newcommand*{\matminus}{%
  \leavevmode
  \hphantom{0}%
  \llap{%
    \settowidth{\dimen0 }{$0$}%
    \resizebox{1.1\dimen0 }{\height}{$-$}%
  }%
}

\title{Notes on computational fluid dynamics}
\author{Cesar B Rocha}
\date{\today}

\begin{document}

\include{mysymbols}

\maketitle

This are notes I've been writing for my self study as part of the class ``Computational Fluid Dynamics'' (MAE 290C), taught by Prof. Juan C. del Alamo in Winter 2015. I'm also including material beyond the class syllabus.


I claim no originality to the content of these notes.  In particular, I'm loosely following del Alamo's class notes, and the following books

\begin{itemize}

    \item \textit{Spectral methods in fluid dynamics} by Canuto et al.;

    \item \textit{Computational techniques in fluid dynamics} by Fletcher et al.;

    \item \textit{Numerical Renaissance} by Bewley.

\end{itemize}

\section{Introduction}
We will study methods for solving  the Navier-Stokes (NS) equations

\beq
\label{eq:ns}
\p_t \vec{u} + \vec{u}\cdot \nabla \vec{u} = -\frac{\nabla p}{\rho} + \nu \lap \vec{u}\com
\eeq
where the laplacian is
\beq
\label{eq:lap_defn}
\lap \defn \nabla \cdot \nabla
\eeq
The momentum equation \eqref{eq:ns} is complemented by the conservation of mass
\beq
\label{eq:cons_mass}
\nabla\cdot\vec{u} = 0\per
\eeq
To close the system, we will also need a thermodynamic equation, which we will introduce later.

The advective term $\vec{u}\cdot\nabla\vec{u}$ gives a hyperbolic flavor to the NS equations. Its quadratic non-linearity typically prohibits analytic solutions. It gives rise to fascinating phenomena like turbulence. In contrast the linear viscous term $\nu\lap\vec{u}$ gives a parabolic flavour the NS equations.

\section{Temporal discretizations of the NS equations}
We consider a one-dimensional linear version of \eqref{eq:ns}, and drop the pressure gradient term for simplicity,
\beq
\label{eq:oned_ns}
\p_t u + c \,\p_x u = \nu \p_{xx}^2 u\com
\eeq
where $c$ is a constant speed. Fourier transforming in the $x$-direction we obtain
\beq
\label{eq:oned_ns_h}
\p_t\hat{u} =\underbrace{ (-\ii c\,k + \nu k^2)}_{\defn \lambda} \hat{u}\com
\eeq
where $k$ is the wavenumber. \eqref{eq:oned_ns} is an eigenproblem with eigenvalue $\lambda$. Decaying solutions have $\Re\left(\lambda\right) < 0$, and numerical schemes must represent this behavior. Figure \ref{fig:eigs_nu} shows the eigenvalue as a function of $\nu$. A $\nu$ increases the real part of the eigenvalue becomes more negative, whereas for very small $\nu$ the eigenvalue are almost purely imaginary. This example illustrates one of the challenges for time marching numerical schemes. These schemes should have stability regions that comprise large negative real number and the imaginary axis. 

\begin{figure}[ht]
\begin{center}
\includegraphics[width=19pc,angle=0]{figs/eigs_linear_1d_ns.eps}
\end{center}
\caption{The eigenvalue $\lambda$ as function of the parameter $\nu$ with fixed speed $c = .5$.}
\label{fig:eigs_nu}
\end{figure}

One may be tempted to use implicit methods since they are unconditionally stable. However, from an implementation point of view, the implicit methods applied to the non-linear terms in \eqref{eq:ns} would lead to non-linear algebraic equations to be solved every time step, which can be very computationally costly. 

For the advective terms we typically use two main schemes:

\begin{itemize}
\item Multi-step (e.g. Adams-Bashfort methods);
    \item Single-step, multiple sub-steps (e.g. Runge-Kutta methods).
\end{itemize}
Adams-Bashforth (AB) methods are more memory demanding, whereas Runge-Kutta (RK) schemes require more flops. The most efficient method depends on computer architecture. In recent computers memory, not CPU, is the main limitations, so we may use RK methods.


\subsection{Runge-Kutta methods}
Runge-Kutta (RK) are explicit, single-step, multi-sub-steps, schemes. The general form of an RK scheme for a non-linear equation 
\beq
\label{eq:gov_eqn}
\p_t u = F(u,t)\com
\eeq
is
\beq
\label{eq:rk_general}
u^{n+1} = u^n + \dt \left(b_1 k_1 + \ldots + b_M k_M\right)
\eeq

\begin{align}
    \label{eq:rk_ks}
    k_1 &= F(u^n,t^n)\com \nonumber \\
k_2 &= F(u^n+a_{21}k_1\dt,t^n + c_2 \dt)\com \nonumber \\
    &\vdots \nonumber \\
    k_{M} &= F(u^n+a_{21}k_1\dt + \ldots + a_{M,M-1}k_{M-1}\dt,t^n , c_2 \dt + \ldots + c_M\dt)\per
\end{align}


To systematically keep track of the coefficients, we introduce the Butcher table
\begin{table}
\label{butcher_table}
\caption{The Butcher table.}
\centering
\begin{tabular}{l| c c c c c c c }
    c$_2$ & a$_{2,1}$ & 0 & \ldots & & 0 \\
    c$_3$ & a$_{3,1}$ & a$_{3,2}$ & 0 & \ldots & $\vdots$ \\
    $\vdots$ & $\vdots$ & $\ddots$ & $\ddots$  \\
    c$_M$ & a$_{M,1}$ & a$_{M,2}$ & &\ldots &a$_{M,M-1}$\\
\hline
\end{tabular}
\end{table}

\subsubsection{RK2, the simplest example}
With M=2 we have
\beq
\label{eq:rk2}
u^{n+1} = u^n + \dt \left(b_1 k_1 + b_2k_2\right)\com
\eeq
with
\begin{align}
k_1 &= F(u^n,t^n ) \com \nonumber \\
k_2 &= F(u^n + a_{2,1} k_1 \dt, t^n +c_2 \dt )\per
\end{align}
There are four parameters $a_{2,1}$, $b_1$, $b_2$, and $c_2$. We choose these parameters to maximize the order of accuracy of the scheme. We expand $k_2$ about $k_1$
\beq
\label{eq:k2_about_k1}
k_2 = k_1 + a_{21} k_1 \dt \,\p_u F (u^n,t^n) + c_2 \dt \p_t \, F + \ord(\dt^2)\per
\eeq
Thus
\beq
\label{eq:un_rk2}
u^{n+1} = u^n + \dt\left[(b_1 + b_2) F(u^n,t^n) + b_2 a_{2,1} \dt\, \p_u F(u^n,t^n) + b_2 c_2 \dt\, \p_t F(u^n,t^n) \right]
\eeq
The Taylor expansion of $u^{n+1}$ about $u^n$ is
\begin{align}
\label{eq:un_taylor}
u^{n+1} = u^n + \dt& \p_t u |^n + \frac{\dt^2}{2} \p_{tt}^n u | ^n + \ord(\dt^3) =\nonumber\\ & u^n + \dt \p_t u |^n +\p_t F(u^n,t^n)+\p_u F(u^n,t^n)F(u^n,t^n) + \ord(\dt^3)  \com
\end{align}
where the equality follows from the governing equation \eqref{eq:gov_eqn}. Matching terms in \eqref{eq:un_taylor} and \eqref{eq:un_rk2} gives
\beq
\label{eq:sys_rk2}
\begin{cases}
b_1 + b_2 = 1; \\
b_2 c_2 = \frac{1}{2};\\
b_2 a_{2,1} = \frac{1}{2}.\\
\end{cases} 
\eeq
The system \eqref{eq:sys_rk2} is underdetermined: there are 4 unknowns but only 3 equations. Choosing $c_2 \equiv c$ as a free parameter, we obtain
\beq
\label{eq:param_rk2}
a_{2,1} = c\com \qquad b_2 = \frac{1}{2c}\com \qqand b_1 = 1 - \frac{1}{2c}\per
\eeq
The parameter $c$ can be chosen to give the best accuracy. Typically $\tfrac{1}{2}\leq c\leq 1$. The linear stability is independent of $c$. For the linear problem $F = \lambda u$. Thus
\beq
\label{eq:stab_rk2}
u^{n+1} = u^n \left(1 + \lambda\dt + \frac{(\lambda\dt)^2}{2}\right)\per
\eeq
Stability requires
\beq
\label{eq:stab_constraint_rk2}
\sigma = \left|1 + \lambda\dt + \frac{(\lambda\dt)^2}{2} \right| \leq 1\per
\eeq
Along the real axis, the stability the stability constrain \eqref{eq:stab_constraint_rk2} is
\beq
\label{eq:stab_real_rk2}
 1 + \lambda_r \dt + \frac{(\lambda_r\dt)^2}{2} \leq 1\com
\eeq
which gives
\beq
-2 \leq \lambda_r \dt \leq 0\per
\eeq
Along the imaginary axis, the stability region only touches $\lambda_i=0$. Figure \ref{fig:stab_region} shows the linear stability region for he RK2 scheme.


\begin{figure}[ht]
\begin{center}
\includegraphics[width=12pc,angle=0]{figs/stability_region_rk2.png}
\includegraphics[width=12pc,angle=0]{figs/stability_region_rk3.png}
\includegraphics[width=12pc,angle=0]{figs/stability_region_rk4.png}
\end{center}
\caption{Stability region for Runge-Kutta schemes.}
\label{fig:stab_region}
\end{figure}


\subsection{RK4}
Derivation of higher-order RK schemes is straightforward but clumsy. A popular scheme is RK4

\beq
\label{rk4}
u^{n+1} = u^n+ \frac{\dt}{6}(  k_1 + 2 k_2 + 2 k_3 + k_4)\com 
\eeq
where
\begin{align}
k_1 &= F(u^n,t^n ) \com \nonumber \\
k_2 &= F(u^n + \half k_1 \dt, t^n + \half \dt )\com \nonumber \\
k_3 &= F(u^n + \half k_2 \dt, t^n + \half \dt )\com \nonumber \\
k_4 &= F(u^n + k_3 \dt, t^n + \dt )\per
\end{align}
This RK4 is one of my favorite time marching  schemes. It is easy to implement, and has good stability properties (see figure \ref{fig:stab_region}).

\subsection{A low-storage RK3 scheme}

To simply notation we write
\beq
\label{eq:lin_nlin}
\p_t u = \sL(u) + \sH(u)\com
\eeq
where $\sL$ comprises the linear terms and $\sH$ contains the nonlinear terms. The low-storage RKW3 scheme is in the form

\beq
\label{eq:rkw3}
u^{n+1} = u^\dstar + \dt [\sL(\alpha_3 u^\star + \beta_3 u^{n+1})] + \gamma_3 \sH(u^\dstar) + \xi_2 \sH(u^\star)\com
\eeq
where
\beq
\label{eq:rkw3_aux}
u^\star = u^n + \dt [\sL(\alpha_1 u^n + \beta_1 u^\star) + \gamma_1 \sH(u^n)]\com
\eeq
and
\beq
\label{eq:rkw3_aux2}
u^\dstar = u^\star + \dt [\sL(\alpha_2 u^\star + \beta_2 u^\dstar) + \gamma_2 \sH(u^\star) + \xi_1 \sH(u^n)]\com
\eeq
with
\beq
\label{eq:rkw3_gammas}
\gamma_1 = \frac{8}{15}\com\qquad \gamma_2 = \frac{5}{12}\com \qqand \gamma_3 = \frac{3}{4}\com
\eeq
\beq
\label{eq:rkw3_betas}
\beta_1 = \frac{37}{160}\com\qquad \beta_2 = \frac{5}{24}\com\qqand \beta_3 = \frac{1}{6}\com
\eeq
\beq
\label{eq:rkw3_alphas}
\alpha_1 = \frac{29}{96}\com\qquad \alpha_2 = -\frac{3}{40}\com\qqand \alpha_3 = \frac{1}{6}\com
\eeq
and
\beq
\label{eq:rkw3_xis}
\xi_1 = -\frac{17}{60}\com\qqand \xi_2 = -\frac{5}{12}\per
\eeq

%Here we only present the results. A popular RK3 scheme is
%\beq
%\label{rk3}
%u^{n+1} = u^n+ \dt(b_1  k_1 + b_2 k_2 + b_3 k_3)\com 
%\eeq
%where
%\begin{align}
%k_1 &= F(u^n,t^n ) \com \nonumber \\
%k_2 &= F(u^n + a_{2,1} k_1 \dt, t^n +c_2 \dt )\com \nonumber \\
%    k_3 &= F(u^n + a_{2,1} k_1 \dt + a_{3,1} k_1 \dt + a_{3,2} k_2 \dt + , t^n +c_2 \dt + c_3 \dt)\com
%\end{align}
%with
%\beq
%b_1=\frac{1}{4}\com \qquad b_2 = 0\com\qqand b_3 = \frac{3}{4}\com
%\eeq
%and
%\beq
%a_{2,1} = \frac{8}{15} \com\qquad a_{3,1} = \frac{1}{4}\com \qqand a_{3,2} = \frac{5}{12}\com
%\eeq
%and
%\beq
%c_2 = \frac{8}{15} \qqand c_3 = \frac{2}{3}\per
%\eeq

\section{$\theta$-schemes}
For the linear viscous terms, it is customary to use implicit schemes. The $\theta$-schemes avoid rapid oscillations for large $\lambda \dt$, while Crank-Nicolson's second-order. These schemes have the form
\beq
\label{eq:th_schemes}
u^{n+1} = u^n + \dt [(1-\theta)F^n + \theta F^{n+1}]\per
\eeq
Notice that CN  is recovered with $\theta = \tfrac{1}{2}$, and implicit Euler is obtain with $\theta = 1$. Typically $\tfrac{1}{2}\leq\theta\leq 1$. 

\subsection*{Linear stability}
The growth factor is
\beq
\label{eq:lin_stab_th}
\sigma = \frac{1 + (1-\theta)\lambda \dt}{1 - \theta \lambda \dt} \per
\eeq
Thus
\beq
\lim_{\lambda\dt\to-\infty} \sigma = -\frac{1-\theta}{\theta} \leq 1 \per
\eeq
With $\theta = \tfrac{3}{4}$, the limit above is $-\tfrac{1}{3}$, a third of CN's limit. 

\subsection*{Order of accuracy}
From the $\theta$-scheme we have
\beq
\frac{u^{n+1}}{u^n} = 1 + \lambda\dt + \theta \lambda^2 \dt^2 + \mathcal{O}(\lambda^3\dt^3).
\eeq
Comparing with the expansion for the exponential $\ee^{\lambda t}$, we conclude that
\beq
\text{Err}_\theta \sim \left(\frac{1}{2}-\theta\right)(\lambda \dt)^2\per 
\eeq
With $\theta = \tfrac{3}{4}$ the error is
\beq
\text{Err}_{3/4} \sim \frac{1}{4}(\lambda \dt)^2\com
\eeq
which is half of the error of the implicit Euler scheme.

% Spectral methods
\section{Spectral methods}
Spectral methods are the golden choice for numerical methods. In problems with simple geometry we express the solution as a linear combination of basis functions
\beq
\label{eq:lin_comb_basis}
u(x_j,t) = \sum_{n=1}^\nmax \hat{u}_n(t) \sb_n(x_j)\com
\eeq
where $b_n(x)$ is the $n$'th basis function. A few desired properties for the basis are

\begin{enumerate}

        
\item Orthogonality 
    \subitem This property ensures accuracy. The basis formed by the set $\{b_n\}$ usually derives from a Sturm-Liouville problem on the domain of interest. The basis functions are typically orthogonal with respect to an inner product $<>$:
\beq
\label{eq:orthogonality}
<\sb_n\,\sb_m> = \alpha_{mn}\,\delta_{mn}\com
\eeq
where $\delta$ is the Kronecker delta, and $\alpha$ is a normalization constant, usually taken to be 1.

The coefficient $\hat{u}_n$ is then found by projection of the true solution on the $n$'th basis function:
\beq
\label{eq:projec}
\hat{u_p} = <\sb_p\,u(x,t)>\per
\eeq

\item Derivatives are known analytically
    \subitem This property allows easy computation of derivatives, e.g.
    \beq
        \label{eq:deriv_basis}
        \p_x u(x_j,t) = \sum_{n=1}^\nmax \hat{u}_n \p_x \sb_n\per
    \eeq
    We assume that the series for the derivative converges within the domain. This is always the case when $u(x,t)$ is periodic and sufficiently smooth within the domain. If $u(x,t)$ has cornes, discontinuities, etc differentiating term-by-term may result in a nonconvergent series. We will return to this issue later.

\item A computational efficient way to transform form the physical domain to the spectral domain.
    \subitem We should be able to easily and efficiently compute $u$ form $\hat{u}_n$, and vice-versa. An example of transform is
    \beq
        \label{eq:ex_transform}
        \hat{u}_n(t) = \frac{1}{\nmax} \sum_{j=1}^{\nmax-1} u(x_j,t) \sb_n(x_j)\com
    \eeq
    Evaluation of the sum in above requires $\mathcal{O}(\nmax)$ operations for every $n$. Thus the cost of the transform is  $\mathcal{O}(\nmax^2)$. For a spectral method to be efficient we should be able to come up with algorithms that reduce this cost, and that can be implemented in parallel. For instance, the celebrated Fast Fourier Transform (FFT)
    reduces this cost to $\mathcal{O}(\nmax \log_2 \nmax)$. This results in a significant reduction of number of operations for large $\nmax$ (see table \ref{tab:nlogn}). For instance, for $\nmax=256$ this results in a saving of $\sim80\%$! Turbulence simulations would not be possible without the  FFT.

\begin{table}
\label{tab:nlogn}
\caption{A comparison of $\nmax^2$ versus $5\nmax\log_2\nmax$}
\centering
\begin{tabular}{l| c c }
    $\nmax$ & $5 \nmax\log_2\nmax$ & $\nmax$ \\
    \hline
    8 & 120 & 64 \\
    16 & 320 & 256\\
    32 & 800 & 1024\\
    64 & 1920 & 4096\\
    128 & 4480 & 16384\\
    256 & 10240 & 65536\\
\hline
\end{tabular}
\end{table}

\end{enumerate}

\subsection*{Fourier spectral methods}
We will focus on the most used class of methods, namely the Fourier spectral methods. In Fourier methods, the set $\{\sb_n\}$ is formed by complex exponentials $\{\exp(\ii \kappa_n x_j)\}$ where
\beq
\label{eq:eigs_fourier}
\kappa_n \defn \frac{2\pi n}{L}\com\qquad n = -\frac{\nmax}{2}\com-\frac{\nmax}{2}+1\com\ldots\com\frac{\nmax}{2}-2\com-\frac{\nmax}{2}-1\per
\eeq
The inverse transform is defined by a truncated Fourier series
\beq
\label{eq:idft}
u(x_j) = \sum_{n=-\nmax/2}^{\nmax/2-1}\!\!\! \hat{u}_n\,\ee^{\ii \kappa_n x_j}\com
\eeq
where $\hat{u}_n$ is the $n$'th Fourier coefficient, which is generally complex-valued. Notice that the series \eqref{eq:idft} implies that $u$ is periodic with period $L$
\beq
\label{eq:idft}
u\left(-\tfrac{L}{2}\right) = u\left(\tfrac{L}{2}\right)\per
\eeq
The discrete Fourier transform (DFT) as
\beq
\label{eq:dft}
\hat{u}_n = \frac{1}{\nmax} \sum_{j=-\nmax/2}^{\nmax/2-1}\!\! u_j \ee^{-\ii \kappa_n x_j}\per
\eeq
We emphasize that one should sum only up to $\nmax/2-1$ since the $u$ is periodic, so that the data at $x_{\nmax/2}$ add no information to the problem.


\subsubsection*{Properties of the DFT}
The discrete Fourier pair \eqref{eq:idft}-\eqref{eq:dft} satisfy the desired properties discussed above. This is the reason for the popularity of Fourier methods. The set $\{\exp(\ii \kappa_n x_j)\}$ forms a complete orthogonal basis on any interval of size $L$ (e.g. $[0,L]$, $[-L/2,L/2]$, etc). Moreover, there exists the FFT algorithm performs the DFT very efficiently. In particular, the FFTW implementation in Fortran and C is very popular.  A summary of important properties is

\begin{enumerate}
    \item As mentioned above the complex exponential are orthogonal with respect to the simplest inner product
        \beq
            \label{eq:orthogonality}
            <\ee^{\ii \kappa_n x}\ee^{\ii \kappa_m x }> \defn \frac{1}{L}\int_{0}^{L}\ee^{\ii \kappa_n x}\ee^{\ii \kappa_m x } \dd x = \delta_{mn}\per
        \eeq
    \item The function $u$ is periodic with period $L$
        \beq
            \label{eq:periodicity_phys}
            u(0) =  \sum_{n=-\nmax/2}^{\nmax/2-1}\!\!\! \hat{u}_n\ee^{\ii \kappa_n\times 0} =  \sum_{n=-\nmax/2}^{\nmax/2-1}\!\!\! \hat{u}_n \ee^{\ii \kappa_n \times L} =  u(L) 
        \eeq

    \item The Fourier coefficients are periodic with period $\nmax$
        \beq
        \label{eq:periodicity_spec}
        \hat{u}_{n+\nmax} = \frac{1}{\nmax} \sum_{j=-\nmax/2}^{\nmax/2-1}\!\! u_j \ee^{-\ii\, \frac{2\pi}{L}(n+\nmax) \frac{j\, L}{\nmax}} = \frac{1}{\nmax} \sum_{j=-\nmax/2}^{\nmax/2-1}\!\! u_j \ee^{-\ii \,2\pi \frac{n\,j}{\nmax}} = \hat{u}_n \per
        \eeq


    \item The zeroth mode represents the average of the function. With $\kappa_0 = 0$, we have
        \beq
            \label{eq:zeroth}
            \hat{u}_0 = \frac{1}{\nmax}\sum_{j=-\nmax/2}^{\nmax}\!\! u_j\per
        \eeq
        Note that some implementations of the DFT do not divide by $\nmax$ in the definition \eqref{eq:idft}. This is simply a matter of definition; one must alway check DocStrings and algorithm manuals to make sure which convention in being used.

\end{enumerate}
   
\subsubsection*{Truncation error, aliasing, and other potential problems}
The definition of the inverse DFT assumes that $u$ has nice properties such as continuity of the function and its derivatives. It the function is discontinuous, then Gibbs oscillation might be introduce Gibbs oscillation, which amount for a $10\%$ error near the discontinuities. The rapid Gibss oscillations can be significantly reduced by using filters.   

A potential problem in spectral methods is aliasing of unresolved high-frequency modes. These modes alias back into low-frequency modes. For linear problems this is not an issue provided that the initial conditions do not contain unresolved high-frequency content. For non-linear problems, such as the NS equations, high-frequency modes are introduced every time step.

\subsubsection*{Parseval's theorem}
The Parseval's relation essentially says that the total energy in physical space is equal to the total energy in Fourier space
\beq
\label{eq:parseval}
\sum_{j=0}^{\nmax-1} u_ju_j^* = \nmax \sum_{n=-\nmax/2}^{\nmax/2-1}\!\! \frac{|\hat{u}_n|^2}{2}\per
\eeq
The proof of this relationship is simple. We introduce the inverse DFT definition \eqref{eq:idft} into \eqref{eq:parseval}
\begin{align}
\label{eq:parseval_1}
\sum_{j=0}^{\nmax-1} u_ju_j^* &= \sum_{j=0}^{\nmax-1} \left( \sum_{n=-\nmax/2}^{\nmax/2-1}\hat{u}_n \ee^{\ii \kappa_n x_j} \right) \left( \sum_{m=-\nmax/2}^{\nmax/2-1}\hat{u}_m^* \ee^{-\ii \kappa_m x_j} \right) \nonumber\\
                              & =\sum_{n=-\nmax/2}^{\nmax/2-1}\sum_{m=-\nmax/2}^{\nmax/2-1}\hat{u}_n \hat{u}_m^*\underbrace{\left(\sum_{j=0}^{\nmax-1} \ee^{\ii(\kappa_n -\kappa_m)x_j}\right)}_{= \nmax \delta_{nm}}\per
\end{align}


\subsubsection*{Truncation error}
To study the truncation error associated with a finite $\nmax$ in \eqref{eq:idft} we consider the true solution
\beq
\label{eq:ift}
u_e = \sum_{n=-\infty}^{\infty}\! \tilde{u}_n \ee^{\ii \kappa_n x_j}\com
\eeq
where the Fourier coefficient is
\beq
\label{eq:ft}
\tilde{u}_n = \frac{1}{L}\int_0^{L}\! u_e(x)\ee^{\ii \kappa_n x}\per
\eeq
Notice that, by aliasing, we have
\beq
\label{eq:uhat_alias}
\hat{u}_n = \tilde{u}_n + \sum_{m=-\infty}^{\infty} \tilde{u}_{n+m\nmax}\per
\eeq
Thus
\beq
\label{eq:idft_2}
u_j = \sum_{n=-\nmax/2}^{\nmax/2-1}\left(\tilde{u}_n + \sum_{m=-\infty}^{\infty}\tilde{u}_{n+m\nmax}\right)\ee^{\ii \kappa_n x_j}\com
\eeq
and the $L_2$-norm of the error is
\beq
\label{eq:error_idft}
\frac{1}{L} \int_0^{L}\left|u_e(x)- \sum_{n=-\nmax/2}^{\nmax/2-1}\!\!\! \hat{u}_n\,\ee^{\ii \kappa_n x_j}\right|^2 \dd x = \underbrace{\sum_{|n|>\nmax/2} |\tilde{u}_n|^2}_{\text{truncation error}} + \underbrace{\sum_{-n=\nmax/2}^{\nmax/2-1}\left| \sum_{m=-\infty}^{\infty}\tilde{u}_{n+m\nmax}\right|^2}_{\text{aliasing error}}\per
\eeq

\subsubsection*{Convergence of the inverse DFT}


\end{document}


